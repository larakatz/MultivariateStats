---
title: 'Lab 2: Multivariate data screening'
author: "Lara Katz"
date: "2024-01-26"
output: word_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE, warning = FALSE)
```

# Set up R session

## Install and load packages.

Note that for a RMarkdown document to knit, you need to comment out these 'install.packages' lines of code before knitting. This is because you cannot knit when your document needs to connect to an external web server. Right now these lines of code are commented out.

```{r eval=FALSE, warning=FALSE, message=FALSE}
# install.packages("mvnormtest")
# install.packages("MVN")
# install.packages("MVA")
# install.packages("psych")
# install.packages("Hmisc")
# install.packages("vegan")
# install.packages("StatMatch")
# install.packages("MASS")
# install.packages("raster")
# install.packages("cluster")
```

```{r warning=FALSE, message=FALSE}
library(mvnormtest)
library(MVN)
library(MVA)
library(psych)
library(Hmisc)
library(vegan)
library(StatMatch)
library(MASS)
library(raster)
library(cluster)
library(tidyverse)
```


## Importing Data

We will be using the `USairpollution` and the `usAir_mod.csv` data sets for these examples.

The MVA US air pollution data set:

```{r}
usAir <- USairpollution
?USairpollution
# SO2: SO2 content of air in micrograms per cubic metre.
# 
# temp: average annual temperature in Fahrenheit.
# 
# manu: number of manufacturing enterprises employing 20 or more workers.
# 
# popul: population size (1970 census); in thousands.
# 
# wind# average annual wind speed in miles per hour.
# 
# precip: average annual precipitation in inches.
# 
# predays: average number of days with precipitation per year.
```

The modified `USairpollution` data set from your working directory is a csv file. Note you will need to modify you working directory if it is different.

```{r}
usAir_mod <- read.csv("./Data/usAir_mod.csv", row=1, header=TRUE)
```

\newline

# Data screening

Your first move when conducting a multivariate analysis (or any analysis) is to screen the data. You are looking for data errors, missing data, and outliers that may influence your analysis.

## Data errors

One way to check for data errors is to examine the summary statistics for your data set.

First look at the summary statistics for `usAir`:

```{r eval=FALSE}
describeBy(usAir)
```

***Question 1: Do you see any unrealistic values? (5 pts) Note please answer all questions with points related to them.***

*The populations of Charleston, SC, and Wilmington, DE, are both below 100 (100,000), which seems low for cities on the east coast. Additionally, Providence, RI, has a high SO2 content despite not having very many large manufacturing enterprises, but this could be a result of pollution from nearby areas or prevailing wind currents.*

Now look at the summary statistics for `usAir_mod`:

```{r eval=FALSE}
describeBy(usAir_mod)
```

Look at the max for temperature. It will be easier to look for data errors when it is your own data.

*The maximum for temperature is now 548.5 degrees Fahrenheit, which is not a realistic value.*

## Missing Data

When you have missing entries in your data sheet, R replaces them with “NA”. You can check if you have any missing variables in *usAir_mod*:

```{r eval=FALSE}
describe(usAir_mod)
```

The *describe* function provides some of the same information as *describeBy*, but importantly shows you which variables have missing values.

We talked about two methods for dealing with missing values in lecture; **Complete Case and Imputation**. We will look at **complete case and imputation** for now.

**Complete Case** involves the removal of samples (in this case cities) with missing data:

```{r eval=FALSE}
usAir_mod [complete.cases(usAir_mod),]
```

**Imputation** involves filling in missing values with plausible data. Let’s replace NAs with the mean of the variable.

```{r}
#First, let’s calculate the mean of each variable (column) with the NA removed:

meanz<-colMeans(usAir_mod,na.rm=T)

#`na.rm=T`, means that you want to remove NAs

#To replace your NAs with the means you just calculated you will use the following function:

naFunc<-function(column) { 
  column[is.na(column)]  = round(mean(column, na.rm = TRUE),2)
  return(column) 
}

#and “apply” it to the usair_mod data set

Impute <- apply(usAir_mod,2,naFunc)

```

Check out the new Impute data object and make sure that the NA’s have been replaced.

```{r}
describe(Impute)
```

We will not go into this advanced function too much. However, know that *apply* allows us to perform a function on all the rows and/or columns in a data frame of matrix. As we spoke about in lecture, there are many types of imputation methods. We can explore further methods for your specific missing data.

# Multivariate Normal Distribution

Many of the analyses we will do in this course have an assumption of multivariate normality. While there are many tests of multivariate normality, they tend to be overly conservative. If we strictly followed these tests, we may never run a multivariate analysis with ecological or agricultural data. Here we will look at two multivariate tests of normality.

## Shapiro-Wilks test

Shapiro-Wilks tests if the distribution of the observed data differs from multivariate normal distribution. So, we are looking for p-values \> 0.05.

```{r}
mshapiro.test(t(usAir))
```

## Mardia test

Mardia’s test looks at multivariate extensions of Skewness and Kurtosis. In both cases, we are looking for p-values \> 0.05 to show that our data do not deviate from the expectations of multivariate normal Skewness and Kurtosis. For the observed data to be considered multivariate normal, p-values from both the Skewness and Kurtosis statistics must be \> 0.05. This function also tests for univariate normality of residuals using the Shapiro-Wilk statistic.

```{r}
mvn(usAir, mvnTest = "mardia")
```

# Data transformation

The next step is preparing your data for analysis is transforming the data. Today we will look at the log, square root, and arcsine square root transformations.

## Log transformation: $$b_{ij}=log(x_{ij}+1)$$

Several common transformations have built-in functions in R. While you can build transformation functions on your own, we will use the ones R has developed today. First, let’s look at a histogram of our first variable, SO2, to determine if transformation is necessary:

Remember, to extract the SO2 column:

```{r eval=FALSE}
usAir$SO2 

#or 

usAir[,1] 


#Next you can simply wrap either of those commands in the histogram function:

hist(usAir$SO2) 

#or 

hist(usAir[,1])  
```

To log transform each value in our data frame:

```{r results='hide'}
usAirlog<-log1p(usAir)
?log1p
```

and the histogram:

```{r eval=FALSE}
hist(usAirlog$SO2) 

#or 

hist(usAirlog[,1])
```

You can compare the histograms side by side using the par function followed by hist:

```{r}
par(mfrow=c(1,2))

hist(usAir[,1])  
hist(usAirlog[,1])  

```

Placing 1, 2 in parentheses after the `c` (which stands for concatenate) in the `par` function indicates that you want your plots arranged in 1 row and two columns. Note this plotting is done in base R as opposed to using the ggplot functions of Tidyverse. It is helpful to know base R and Tidyverse to be able to read and trouble shoot code with a wide range of collaborators. In ggplot this code would be similar to what the *facet* function does.

Compare histograms for the raw data and the log transformed data for each variable.

`temp`:
```{r}
par(mfrow=c(1,2))

hist(usAir[,2])  
hist(usAirlog[,2]) 
```

`manu`:
```{r}
par(mfrow=c(1,2))

hist(usAir[,3])  
hist(usAirlog[,3]) 
```

`popul`:
```{r}
par(mfrow=c(1,2))

hist(usAir[,4])  
hist(usAirlog[,4]) 
```

`wind`:
```{r}
par(mfrow=c(1,2))

hist(usAir[,5])  
hist(usAirlog[,5]) 
```

`precip`:
```{r}
par(mfrow=c(1,2))

hist(usAir[,6])  
hist(usAirlog[,6]) 
```

`predays`:
```{r}
par(mfrow=c(1,2))

hist(usAir[,7])  
hist(usAirlog[,7]) 
```

**Question 2: Which variable might not need to be log transformed? (5 pts)**

*The `wind` and `predays` variables are both normally distributed and may not need to be log-transformed.*

## Square root transformation: $$b_{ij}=\sqrt{x_{ij}}$$

To square root transform each value in our data frame:

```{r results='hide'}
usAirsqrt<-sqrt(usAir)
```

and the histogram:

```{r eval=FALSE}
hist(usAirsqrt$SO2)

#or 

hist(usAirsqrt[,1])  
```

Compare the histograms side by side using the par function followed by hist:

```{r}
par(mfrow=c(1,2))

hist(usAir[,1])  
hist(usAirsqrt[,1]) 
```

Compare histograms for the raw data and the square-root transformed data for each variable…

Remember that square root transformations are best used on count data.

`temp`:
```{r}
par(mfrow=c(1,2))

hist(usAir[,2])  
hist(usAirsqrt[,2]) 
```

`manu`:
```{r}
par(mfrow=c(1,2))

hist(usAir[,3])  
hist(usAirsqrt[,3]) 
```

`popul`:
```{r}
par(mfrow=c(1,2))

hist(usAir[,4])  
hist(usAirsqrt[,4]) 
```

`wind`:
```{r}
par(mfrow=c(1,2))

hist(usAir[,5])  
hist(usAirsqrt[,5]) 
```

`precip`:
```{r}
par(mfrow=c(1,2))

hist(usAir[,6])  
hist(usAirsqrt[,6]) 
```

`predays`:
```{r}
par(mfrow=c(1,2))

hist(usAir[,7])  
hist(usAirsqrt[,7]) 
```

## Arcsine square root transformation: $$b_{ij}$$ = arcsine$$\sqrt{x_{ij}}$$

If you remember arcsine square root transformations are for percentage data. So, the values for your variable must range between 0 and 1. None of the variables in `usAir` are appropriate for this transformation. Let’s draw some random numbers between 0 and 1 so we can use the arcsine square root transformation.

```{r}
newData<- runif(100, 0, 1)
```

You just chose 100 random values between 0 and 1. Now let’s transform:

```{r eval=FALSE}
asin(sqrt(newData))
```

and compare histograms:

```{r}
par(mfrow=c(1,2))

hist(newData)
hist(asin(sqrt(newData)))
```

# Data standardization

Column standardization adjusts for differences among variables. The focus is on the profile across a sample unit. Row standardization adjusts for differences among sample units, wherein the focus is on the profile within a sample unit. Row standardization is good when variables are measured in the same units (e.g. species). You will more often than not be using column standardization.

## Coefficient of Variation (cv)

Let’s first see if the air pollution data set needs standardization by calculating the *coefficient of variation* **(cv)** for column totals. Remember, the **cv** is the ratio of the standard deviation to the mean (σ/μ):

First calculate the column **sums**:

```{r results='hide'}
cSums<-colSums(usAir)
```

Then calculate the **standard deviation** and **mean** for the column sums:

```{r results='hide'}
Sdev <- sd(cSums)
M <- mean(cSums)
```

Finally, calculate the **cv**:

```{r}
Cv <- Sdev/M*100
```

Our rule of thumb for cv is that if **cv\> 50**, data standardization is necessary.

***Question 3: Is standardization necessary for the `USairpollution` data? (5 pts)***

*Data standardization is necessary because the CV of the dataset is 129.27.*

## Z- standardization $$b_{ij}$$ = ($$x_{ij}- \bar{x_{j}})/s_{j}$$

Your goal here is to equalize the variance for variables measured on different scales. There is a built-in function `scale` that will do this for you:

```{r results='hide'}
scaledData <- scale(usAir)
```

Let’s look at histograms for the scaled and unscaled data for the first variable, SO2:

```{r}
par(mfrow=c(1,2))

hist(usAir[,1] ,main=colnames(usAir)[1],xlab=" ")
hist(scaledData[,1] ,main=colnames(usAir)[1],xlab=" ")

mean(scaledData[,1])
var(scaledData[,1])
```

Compare the raw and standardized histograms for all of the variables.

```{r}
par(mfrow=c(2,4))
hist(usAir [,1] ,main=colnames(usAir)[1],xlab=" ")
hist(scaledData [,1] ,main=colnames(usAir)[1],xlab=" ")
hist(usAir [,2] ,main=colnames(usAir)[2],xlab=" ")  
hist(scaledData [,2] ,main=colnames(usAir)[2],xlab=" ")  
hist(usAir [,3] ,main=colnames(usAir)[3],xlab=" ")  
hist(scaledData [,3] ,main=colnames(usAir)[3],xlab=" ")  
hist(usAir [,4] ,main=colnames(usAir)[4],xlab=" ")
hist(scaledData [,4] ,main=colnames(usAir)[4],xlab=" ")

par(mfrow=c(2,4))
hist(usAir [,5] ,main=colnames(usAir)[5],xlab=" ")  
hist(scaledData [,5] ,main=colnames(usAir)[5],xlab=" ")  
hist(usAir [,6] ,main=colnames(usAir)[6],xlab=" ")  
hist(scaledData [,6] ,main=colnames(usAir)[6],xlab=" ")  
hist(usAir [,7] ,main=colnames(usAir)[7],xlab=" ")
hist(scaledData [,7] ,main=colnames(usAir)[7],xlab=" ")
```


***Question 4: Are you convinced that the variances are equalized? Just to check, calculate the mean and variance for each of the standardized variables. (10 pts)***

*The variances have all been equalized to 1, and the means are very close to zero but not exactly zero*:
```{r}
# temperature
mean(scaledData[,2]) 
var(scaledData[,2])

# manufacturing
mean(scaledData[,3])
var(scaledData[,3])

# population
mean(scaledData[,4])
var(scaledData[,4])

# wind
mean(scaledData[,5])
var(scaledData[,5])

# precip
mean(scaledData[,6])
var(scaledData[,6])

#predays
mean(scaledData[,7])
var(scaledData[,7])
```

**Z standardization is very common in life sciences.**

# Detecting Outliers

Outliers are recorded values of measurements or observations that are outside the range of the bulk of the data. Outliers can inflate variance and lead to erroneous conclusions.

## Univariate outliers

One way to deal with outliers in multivariate data is to examine each variable separately. You will standardize your data into standard deviation units (z –standardization) and look for values that fall outside of three standard deviations.

First the z-standardization:

```{r results='hide'}
scaledData <- scale(usAir)
```

Next we will create histograms to look for values \> than 3 sd. However, this time we will use the *par function* to look at all seven histograms at once.

```{r eval=FALSE}
par(mfrow=c(2,4))
hist(scaledData [,1] ,main=colnames(usAir)[1],xlab=" ")
hist(scaledData [,2] ,main=colnames(usAir)[2],xlab=" ")  
hist(scaledData [,3] ,main=colnames(usAir)[3],xlab=" ")  
hist(scaledData [,4] ,main=colnames(usAir)[4],xlab=" ")
hist(scaledData [,5] ,main=colnames(usAir)[5],xlab=" ")  
hist(scaledData [,6] ,main=colnames(usAir)[6],xlab=" ")  
hist(scaledData [,7] ,main=colnames(usAir)[7],xlab=" ")
```

Finally, you can identify the outlier(s) for each variable:

```{r eval=FALSE}
scaledData [,1][scaledData [,1]>3] 
scaledData [,2][scaledData [,2]>3]  
scaledData [,3][scaledData [,3]>3] 
scaledData [,4][scaledData [,4]>3]
scaledData [,5][scaledData [,5]>3]
scaledData [,6][scaledData [,6]>3]  
scaledData [,7][scaledData [,7]>3]
```

Alternatively, you could use the apply function, less typing!

For the histogram function (hist):

```{r eval=FALSE}
par(mfrow=c(2,4))
mapply(hist,as.data.frame(usAir),main=colnames(usAir),xlab=" ")
```

Here is a new function for detecting outliers called out.

```{r}
out<-function(x){
lier<-x[abs(x)>3]
return(lier)
}
```

Let’s apply that function:

```{r}
apply(scaledData,2,out)
```

**Question 5: Do you detect any outliers? For which variables? (5 pts)**

*I detected 1 outlier in the `SO2`, `manu`, and `popul` columns. All of these outliers came from the Chicago row.*

## Multivariate outliers

**we will come back to this...**

# Distance and Dissimilarity

As we know from lecture, multivariate data with *p* variables are visually represented by a collection of points forming a data cloud in *p*-dimensional space. The shape, clumping, and dispersion of the data cloud contains information we seek to describe. Several distance and dissimilarity measures are used to calculate the distance between data points.

## Euclidean Distance:

**Euclidean** distance is one of the most commonly used distance measures. It is normally preceded by column standardization (e.g. z standardization). Let's calculate Euclidean distance for the US air pollution data set. You will use the function *`vegdist`* from the *vegan* (vegetation analysis) package. Look up *`vegdist`* to see the different indices available in this package.

```{r eval=FALSE}
?vegdist 
```

First, z standardization:

```{r results='hide'}
scaledData <- scale(usAir)
```

Then calculate distance:

```{r results='hide'}
eucDist <- vegdist(scaledData,"euclidean")

```

Let’s look at a histogram of distances:

```{r}
hist(eucDist)
mean(eucDist)
max(eucDist)
```

**Question 6: What does this frequency distribution tell you about pollution conditions across these 41 cities? (5 pts)**

*The histogram tells us that most cities are fairly close to one another in the data cloud. The majority of observations are clustered between 1 and 5 on the histogram, with a mean of 3.36. The distribution of Euclidean distances is left-skewed, and the maximum Euclidean distance is 10.25.*

Euclidean Distance can be weird. Let look at the data matrix below:

We want to determine how similar these farms are in theit production of strawberries, peaches, and raspberries.

```{r}
Fruit <-rbind(c(1,0,1,1),c(2,1,0,0), c(3,0,4,4))
colnames(Fruit)<-c("Farm","Strawberry","Peach", "Raspberry")
Fruit
```

Calculating Euclidean distance on these data:

```{r}
eucDist<- vegdist(Fruit[,-1], "euclidean")
```

Gives us this distance matrix (R gives you the triangular matrix without the diagonal):

The distance between farms 1 and 2, which grow none of the same fruits:

$$d_{1,2}=\sqrt{((1-0)^2 )+(1-0)^2+(1-0)^2}=1.732$$

Is **less** (i.e., these farms are more similar in their fruit production) than farms 1 and 3, which grow the same fruit:

$$d_{1,3}=\sqrt{((0-0)^2 )+(1-4)^2+(1-4)^2}=4.234$$

Euclidean distance is not a jack-of-all-trades and is not appropriate for all data sets. Our next distance metric, Manhattan distance would also rank Farms 1 and 2 more similar than 1 and 3.

## City-block (Manhattan) distance

```{r}
cbDist <- vegdist(scaledData,"manhattan")

#Let’s look at a histogram of distances:

hist(cbDist)
mean(cbDist)
max(cbDist)
```

**Question 7: How does this distribution compare to Euclidean distance? (5 pts)**

*This histogram is also left-skewed. The mean (7.13) is higher than that of the Euclidean distance distribution, and the maximum distance is also higher (26.01).*

## Bray-Curtis dissimilarity

```{r}
brayDist <- vegdist(usAir,"bray")

#Histogram:

hist(brayDist)
```

Let’s quickly look at our fruit farm data with Bray-Curtis:

```{r}
brayFruit<- vegdist(Fruit[,-1], "bray")
brayFruit
```

That makes more sense! Farms 1 and 2 (and 2 and 3) are at maximum dissimilarity and farms 1, 3 are more similar.

**Back to multivariate outliers!**

Your goal here is to examine deviations of the sample average distances to other samples. We will use **Bray-Curtis** distance:

```{r}
brayDist <- vegdist(usAir,"bray")
```

Next, calculate column means. These column means represent the average dissimilarity of each city to all other cities. You want to know if any cities are on average more than 3 standard deviation units (z scores). To achieve this, z-transform the averages:

```{r}
multOut <- scale(colMeans(as.matrix(brayDist)))
```

Plot a histogram and look for observations \>3 sd units:

```{r}
hist(multOut)
```

You can find the cities that are outliers with:

```{r}
multOut [multOut >3,]
```

Another possibility is to determine which observation are \> 3 standard deviations from the mean. Using Bray-Curtis distance again:

Calculate column means:

```{r}
colBray <- colMeans(as.matrix(brayDist))
```

Calculate the mean of the column means:

```{r}
mBray <- mean(colBray)
```

Calculate the standard deviation:

```{r}
stdBray <- sd(colBray)
```

… 3 standard deviations

```{r}
threeSD <- stdBray * 3 + mBray
```

plot a histogram and look for observations \>3 sd:

```{r }
hist(colBray)
```

Find the outliers:

```{r}
colBray [colBray >threeSD]
```

# Bridle Shiner data

**Question 8: NOW, RUN THROUGH THE ABOVE EXERCISES WITH YOUR OWN DATA! (55 pts)**

## Import Data

```{r}
bds <- read.csv("./Data/Katz_BDS_data.csv", header = TRUE, row.names = 1)
bds <- bds[,-2]
bds$catchment <- as.factor(bds$catchment)
bds$lith <- as.factor(bds$lith)
bds$marine <- as.factor(bds$marine)
bds$ph <- bds$ph/10 # raster has pH values * 10 so that values are integers
```


## Data screening

Your first move when conducting a multivariate analysis (or any analysis) is to screen the data. You are looking for data errors, missing data, and outliers that may influence your analysis.

### Data errors

One way to check for data errors is to examine the summary statistics for your data set.

First look at the summary statistics for `bds`:

```{r eval=FALSE}
describeBy(bds)
```

***Question 1: Do you see any unrealistic values? (5 pts) Note please answer all questions with points related to them.***

*While no values appear to be unrealistic, some variables are highly skewed or have very high kurtosis values. This is because some land cover types are relatively rare in the state, so these columns are zero-inflated.*

### Missing Data

When you have missing entries in your data sheet, R replaces them with “NA”. You can check if you have any missing variables in *bds*:

```{r eval=FALSE}
describe(bds)
```
*There are no `NA` values in the `bds` dataset because the function that generated the random points removed points with `NA` values.*

The *describe* function provides some of the same information as *describeBy*, but importantly shows you which variables have missing values.

We talked about two methods for dealing with missing values in lecture; **Complete Case and Imputation**. We will look at **complete case and imputation** for now.

**Complete Case** involves the removal of samples (in this case cities) with missing data:

```{r eval=FALSE}
bds [complete.cases(bds),] # all cases are complete cases
```

**Imputation** involves filling in missing values with plausible data. Let’s replace NAs with the mean of the variable.

```{r}
# Remove columns with categorical or binary data
bds.num <- bds %>% dplyr::select(-pab, -catchment, -lith, -marine)

#First, let’s calculate the mean of each variable (column) with the NA removed:

meanz <- colMeans(bds.num,na.rm=T)

#`na.rm=T`, means that you want to remove NAs

#To replace your NAs with the means you just calculated you will use the following function:

naFunc<-function(column) { 
  column[is.na(column)]  = round(mean(column, na.rm = TRUE),2)
  return(column) 
}

#and “apply” it to the usair_mod data set

Impute <- apply(bds.num,2,naFunc)

```

Check out the new Impute data object and make sure that the NA’s have been replaced.

```{r, eval=FALSE}
describe(Impute) # same as original dataset since there were no NA values
```

We will not go into this advanced function too much. However, know that *apply* allows us to perform a function on all the rows and/or columns in a data frame of matrix. As we spoke about in lecture, there are many types of imputation methods. We can explore further methods for your specific missing data.

## Multivariate Normal Distribution

Many of the analyses we will do in this course have an assumption of multivariate normality. While there are many tests of multivariate normality, they tend to be overly conservative. If we strictly followed these tests, we may never run a multivariate analysis with ecological or agricultural data. Here we will look at two multivariate tests of normality.

### Shapiro-Wilks test

Shapiro-Wilks tests if the distribution of the observed data differs from multivariate normal distribution. So, we are looking for p-values \> 0.05.

```{r}
#mshapiro.test(t(bds.num)) # sample size is too large for the Shapiro-Wilks test
bds.5000 <- sample(nrow(bds.num), 5000) # random sample of points
mshapiro.test(t(bds.5000))
```

### Mardia test

Mardia’s test looks at multivariate extensions of Skewness and Kurtosis. In both cases, we are looking for p-values \> 0.05 to show that our data do not deviate from the expectations of multivariate normal Skewness and Kurtosis. For the observed data to be considered multivariate normal, p-values from both the Skewness and Kurtosis statistics must be \> 0.05. This function also tests for univariate normality of residuals using the Shapiro-Wilk statistic.

```{r}
mvn(bds.num, mvnTest = "mardia")
```

## Data transformation

The next step is preparing your data for analysis is transforming the data. Today we will look at the log, square root, and arcsine square root transformations.

### Log transformation: $$b_{ij}=log(x_{ij}+1)$$

Several common transformations have built-in functions in R. While you can build transformation functions on your own, we will use the ones R has developed today. First, let’s look at a histogram of our first variable, *clay*, to determine if transformation is necessary:

```{r eval=FALSE}
hist(bds.num$clay) 
```

To log transform each value in our data frame:

```{r results='hide'}
bdslog <- log1p(bds.num)
```

and the histogram:

```{r eval=FALSE}
hist(bdslog$clay) 
```

You can compare the histograms side by side using the par function followed by hist:

```{r}
par(mfrow=c(1,2))

hist(bds.num[,1])  
hist(bdslog[,1])  
```

Placing 1, 2 in parentheses after the `c` (which stands for concatenate) in the `par` function indicates that you want your plots arranged in 1 row and two columns. Note this plotting is done in base R as opposed to using the ggplot functions of Tidyverse. It is helpful to know base R and Tidyverse to be able to read and trouble shoot code with a wide range of collaborators. In ggplot this code would be similar to what the *facet* function does.

Compare histograms for the raw data and the log transformed data for each variable.

```{r}
par(mfrow=c(2,4))
for (i in 1:ncol(bds.num)){
  hist(bds.num[,i], main = paste(colnames(bds.num)[i]), xlab = "")
  hist(bdslog[,i], main = paste(colnames(bdslog)[i]), xlab = "")
}
```


**Question 2: Which variable might not need to be log transformed? (5 pts)**

*All of the variables need to be transformed in some way because none of them are normally distributed. The variables are also measured in different units: the soil data (e.g., clay, silt, and sand) were measured in g/kg, but the land cover types are areal proportions.*


### Square root transformation: $$b_{ij}=\sqrt{x_{ij}}$$

To square root transform each value in our data frame:

```{r results='hide'}
bdssqrt <- sqrt(bds.num)
```

and the histogram:

```{r eval=FALSE}
hist(bdssqrt$clay)
```

Compare the histograms side by side using the par function followed by hist:

```{r}
par(mfrow=c(1,2))

hist(bds.num[,1])  
hist(bdssqrt[,1]) 
```

Compare histograms for the raw data and the square-root transformed data for each variable…

Remember that square root transformations are best used on count data.


```{r}
par(mfrow=c(2,4))
for (i in 1:ncol(bds.num)){
  hist(bds.num[,i], main = paste(colnames(bds.num)[i]), xlab = "")
  hist(bdssqrt[,i], main = paste(colnames(bdssqrt)[i]), xlab = "")
}
```


### Arcsine square root transformation: $$b_{ij}$$ = arcsine$$\sqrt{x_{ij}}$$

If you remember arcsine square root transformations are for percentage data. So, the values for your variable must range between 0 and 1. All of the LANDFIRE land cover variables in `bds` are appropriate for this transformation. 

```{r}
bds.lf <- bds.num[,c(5:31)]
bds.lf <- bds.lf/100 # to percentages between 0 and 1
```

Now let’s transform:

```{r eval=FALSE}
asin(sqrt(bds.lf))
```

and compare histograms:

```{r}
par(mfrow=c(1,2))

hist(bds.lf$X10_Ap_Hemlock.N_Hardwood)
hist(asin(sqrt(bds.lf$X10_Ap_Hemlock.N_Hardwood)))
```
```{r}
par(mfrow=c(2,4))
for (i in 1:ncol(bds.lf)){
  hist(bds.lf[,i], main = colnames(bds.lf)[i], xlab = "")
  hist(asin(sqrt(bds.lf[,i])), main = colnames(bds.lf)[i], xlab = "")
}
```

## Data standardization

Column standardization adjusts for differences among variables. The focus is on the profile across a sample unit. Row standardization adjusts for differences among sample units, wherein the focus is on the profile within a sample unit. Row standardization is good when variables are measured in the same units (e.g. species). You will more often than not be using column standardization.

### Coefficient of Variation (cv)

Let’s first see if the bridle shiner data set needs standardization by calculating the *coefficient of variation* **(cv)** for column totals. Remember, the **cv** is the ratio of the standard deviation to the mean (σ/μ):

First calculate the column **sums**:

```{r results='hide'}
cSums <- colSums(bds.num)
```

Then calculate the **standard deviation** and **mean** for the column sums:

```{r results='hide'}
Sdev <- sd(cSums)
M <- mean(cSums)
```

Finally, calculate the **cv**:

```{r}
Cv <- Sdev/M*100
```

Our rule of thumb for cv is that if **cv\> 50**, data standardization is necessary.

***Question 3: Is standardization necessary for the `USairpollution` data? (5 pts)***

*Data standardization is necessary because the CV of the dataset is 270.94*

### Z- standardization $$b_{ij}$$ = ($$x_{ij}- \bar{x_{j}})/s_{j}$$

Your goal here is to equalize the variance for variables measured on different scales. There is a built-in function `scale` that will do this for you:

```{r results='hide'}
scaledData <- scale(bds.num)
```

Let’s look at histograms for the scaled and unscaled data for the first variable, clay:

```{r}
par(mfrow=c(1,2))

hist(bds.num[,1] ,main=colnames(bds.num)[1],xlab=" ")
hist(scaledData[,1] ,main=colnames(bds.num)[1],xlab=" ")

mean(scaledData[,1])
var(scaledData[,1])
```

Compare the raw and standardized histograms for all of the variables.
```{r}
par(mfrow=c(2,4))
for (i in 2:ncol(bds.num)){
  hist(bds.num[,i], main = paste(colnames(bds.num)[i]), xlab=" ")
  hist(scaledData[,i], main = paste(colnames(bds.num)[i]), xlab=" ")
}
```


***Question 4: Are you convinced that the variances are equalized? Just to check, calculate the mean and variance for each of the standardized variables. (10 pts)***

*The variances have all been equalized to 1, and the means are close to zero but not exactly zero*:
```{r}
q4 <- data.frame(matrix(NA, nrow=ncol(bds.num), ncol = 2))
colnames(q4) <- c("mean", "variance")
rownames(q4) <- colnames(bds.num)

for (i in 1:ncol(bds.num)){
  q4[i,1] <- mean(scaledData[,i])
  q4[i,2] <- var(scaledData[,i])
}
min(q4$variance)
max(q4$variance)
min(q4$mean)
max(q4$mean)
```

**Z standardization is very common in life sciences.**

## Detecting Outliers

Outliers are recorded values of measurements or observations that are outside the range of the bulk of the data. Outliers can inflate variance and lead to erroneous conclusions.


### Univariate outliers

One way to deal with outliers in multivariate data is to examine each variable separately. You will standardize your data into standard deviation units (z –standardization) and look for values that fall outside of three standard deviations.

First the z-standardization:

```{r results='hide'}
scaledData <- scale(bds.num)
```

Next we will create histograms to look for values \> than 3 sd. However, this time we will use the *par function* to look at all seven histograms at once.

```{r eval=FALSE}
par(mfrow=c(2,4))
for (i in 1:ncol(bds.num)){
  hist(scaledData[,i], main = colnames(bds.num)[i], xlab = "")
}
```

Finally, you can identify the outlier(s) for each variable:

```{r eval=FALSE}
for (i in 1:ncol(bds.num)){
  print(scaledData[,i][scaledData[,i] > 3])
}
```

Alternatively, you could use the apply function, less typing!

For the histogram function (hist):

```{r eval=FALSE}
par(mfrow=c(2,4))
mapply(hist,as.data.frame(scaledData),main=colnames(bds.num),xlab = "")
```

Here is a new function for detecting outliers called out.

```{r}
out<-function(x){
lier<-x[abs(x)>3]
return(lier)
}
```

Let’s apply that function:

```{r}
apply(scaledData,2,out)
```

**Question 5: Do you detect any outliers? For which variables? (5 pts)**

*I detected many outliers across all of the variables except for `coast`, `X01_water`, `X03_LAc_NHardwd`, and `X10_Ap_Hemlock.N_Hardwood`.*

### Multivariate outliers

**we will come back to this...**

## Distance and Dissimilarity

As we know from lecture, multivariate data with *p* variables are visually represented by a collection of points forming a data cloud in *p*-dimensional space. The shape, clumping, and dispersion of the data cloud contains information we seek to describe. Several distance and dissimilarity measures are used to calculate the distance between data points.

### Euclidean Distance:

**Euclidean** distance is one of the most commonly used distance measures. It is normally preceded by column standardization (e.g. z standardization). Let's calculate Euclidean distance for the US air pollution data set. You will use the function *`vegdist`* from the *vegan* (vegetation analysis) package. Look up *`vegdist`* to see the different indices available in this package.

```{r eval=FALSE}
?vegdist 
```

First, z standardization:

```{r results='hide'}
scaledData <- scale(bds.num)
```

Then calculate distance:

```{r results='hide'}
eucDist <- vegdist(scaledData,"euclidean")
```

Let’s look at a histogram of distances:

```{r}
hist(eucDist)
mean(eucDist)
max(eucDist)
```

**Question 6: What does this frequency distribution tell you about pollution conditions across these 41 cities? (5 pts)**

*The histogram tells us that most sites/points are under 20 units away from one another in the data cloud. The majority of observations are clustered between approximately 5 and 10 on the histogram, with a mean of 7.47. The distribution of Euclidean distances is left-skewed, and the maximum Euclidean distance is 58.43.*

### City-block (Manhattan) distance

```{r}
cbDist <- vegdist(scaledData,"manhattan")

#Let’s look at a histogram of distances:

hist(cbDist)
mean(cbDist)
max(cbDist)
```

**Question 7: How does this distribution compare to Euclidean distance? (5 pts)**

*This histogram is also left-skewed. The mean (24.20) is higher than that of the Euclidean distance distribution, and the maximum distance is much higher (122.09).*

### Bray-Curtis dissimilarity

```{r}
brayDist <- vegdist(bds.num,"bray")

#Histogram:

hist(brayDist)
```

**Back to multivariate outliers!**

Your goal here is to examine deviations of the sample average distances to other samples. We will use **Bray-Curtis** distance:

```{r}
brayDist <- vegdist(bds.num,"bray")
```

Next, calculate column means. These column means represent the average dissimilarity of each city to all other cities. You want to know if any cities are on average more than 3 standard deviation units (z scores). To achieve this, z-transform the averages:

```{r}
multOut <- scale(colMeans(as.matrix(brayDist)))
```

Plot a histogram and look for observations \>3 sd units:

```{r}
hist(multOut)
```

You can find the points that are outliers with:

```{r}
multOut [multOut >3,]
```

Another possibility is to determine which observation are \> 3 standard deviations from the mean. Using Bray-Curtis distance again:

Calculate column means:

```{r}
colBray <- colMeans(as.matrix(brayDist))
```

Calculate the mean of the column means:

```{r}
mBray <- mean(colBray)
```

Calculate the standard deviation:

```{r}
stdBray <- sd(colBray)
```

… 3 standard deviations

```{r}
threeSD <- stdBray * 3 + mBray
```

plot a histogram and look for observations \>3 sd:

```{r }
hist(colBray)
```

Find the outliers:

```{r}
outliers <- colBray[colBray > threeSD]
length(outliers)
```

*There are 193 outlier points/sites.*